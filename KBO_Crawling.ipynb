{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawling_redo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkr9uxhBR4DT",
        "outputId": "96f0e208-5889-43cc-bf11-b11a4078b2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 34.6 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-JkhaqoSAgo",
        "outputId": "51c42758-a9f5-4557-ba7b-6d68caef375f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Waiting for headers] [2 InRelease 14.2 kB/88.7 kB 16%] [Connected to cloud.\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [2 InRelease 14.2 kB/88.7 kB\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 242 kB] [3 InRelease 14.2 kB/88.7 kB 16%] [2 InRelease 40.\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [1 InRelease gpgv 242 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [2 InRelease 43.\r0% [1 InRelease gpgv 242 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [2 InRelease 43.\r0% [1 InRelease gpgv 242 kB] [3 InRelease 47.5 kB/88.7 kB 54%] [Waiting for hea\r                                                                               \r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers]\r                                                                        \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [6 InRelease 14.2 kB/15.9 kB\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,512 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,799 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,231 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,286 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,992 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,021 kB]\n",
            "Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 13.2 MB in 5s (2,847 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 3s (27.8 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install html_table_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKGu8sd0SGpx",
        "outputId": "ab36fb73-db1f-431d-a29e-7333588ac781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting html_table_parser\n",
            "  Downloading html_table_parser-0.1.0.tar.gz (3.4 kB)\n",
            "Collecting beautifulsoup4==4.4.1\n",
            "  Downloading beautifulsoup4-4.4.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 3.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: html-table-parser\n",
            "  Building wheel for html-table-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html-table-parser: filename=html_table_parser-0.1.0-py3-none-any.whl size=4645 sha256=cdbe03f1e5ae8e4f44ea24e5acc2716d04670d1576d2a9a8cdf40c6e5d04809b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/db/ba/ffbb0a5dcb78fd955a33d26295a77ced593e712a7d8a4a0dc6\n",
            "Successfully built html-table-parser\n",
            "Installing collected packages: beautifulsoup4, html-table-parser\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.4.1 html-table-parser-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade beautifulsoup4\n",
        "!pip install --upgrade html5lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgjsLB13YiA-",
        "outputId": "64cb5434-23c9-4996-fac6-2066b09a1463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4) (2.3.2.post1)\n",
            "Installing collected packages: beautifulsoup4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.4.1\n",
            "    Uninstalling beautifulsoup4-4.4.1:\n",
            "      Successfully uninstalled beautifulsoup4-4.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "html-table-parser 0.1.0 requires beautifulsoup4==4.4.1, but you have beautifulsoup4 4.11.1 which is incompatible.\u001b[0m\n",
            "Successfully installed beautifulsoup4-4.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Collecting html5lib\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from html5lib) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib) (0.5.1)\n",
            "Installing collected packages: html5lib\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "Successfully installed html5lib-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfUqLF1oRSA8",
        "outputId": "0c2d992a-bcbc-4f5a-8242-6daf157a196d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "import urllib.request\n",
        "from html_table_parser import parser_functions\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n",
        " \n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver', options=options)"
      ],
      "metadata": {
        "id": "8cO3luo-THwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 타자 크롤링"
      ],
      "metadata": {
        "id": "DIIS08r4ScHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batter_url =  'http://www.statiz.co.kr/stat.php?opt=0&sopt=0&re=0&ys={}&ye={}&se=0&te=&tm=&ty=0&qu=auto&po=0&as=&ae=&hi=&un=&pl=&da=1&o1=WAR_ALL_ADJ&o2=TPA&de=1&tr=&cv=&ml=1&sn=400&si=&cn='"
      ],
      "metadata": {
        "id": "flCgC6FDSa-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batter_url = batter_url.format(2017,2017)\n",
        "driver.get(batter_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2017}년도_{\"타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "ywhDsWlyRwPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batter_url = batter_url.format(2018,2018)\n",
        "driver.get(batter_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2018}년도_{\"타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "aYnr_2TPTwJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batter_url = batter_url.format(2019,2019)\n",
        "driver.get(batter_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2019}년도_{\"타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "_iAEH5gQT2NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batter_url = batter_url.format(2020,2020)\n",
        "driver.get(batter_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2020}년도_{\"타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "wJG8xEU0U7t3",
        "outputId": "319f31b5-9c9b-49b0-93f3-e35943b76ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-030c604de593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatter_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatter_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatter_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'mytable'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batter_url' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batter_url = batter_url.format(2021,2021)\n",
        "driver.get(batter_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2021}년도_{\"타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "ZCjcUnqKZyGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 투수 크롤링"
      ],
      "metadata": {
        "id": "XyAH045Nnq2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher_url = 'http://www.statiz.co.kr/stat.php?opt=0&sopt=0&re=1&ys={}&ye={}&se=0&te=&tm=&ty=0&qu=auto&po=0&as=&ae=&hi=&un=&pl=&da=1&o1=WAR&o2=OutCount&de=1&tr=&cv=&ml=1&sn=400&si=&cn='"
      ],
      "metadata": {
        "id": "4JUvyQx0aneP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher_url = pitcher_url.format(2017,2017)\n",
        "driver.get(pitcher_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2017}년도_{\"투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "oqKtWjUFnx_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher_url = pitcher_url.format(2018,2018)\n",
        "driver.get(pitcher_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2018}년도_{\"투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "eP2JCu5in6H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher_url = pitcher_url.format(2019,2019)\n",
        "driver.get(pitcher_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2019}년도_{\"투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "a30pfdWpn7he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher_url = pitcher_url.format(2020,2020)\n",
        "driver.get(pitcher_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2020}년도_{\"투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "uvawuDtcn8LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitcher_url = pitcher_url.format(2021,2021)\n",
        "driver.get(pitcher_url)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2021}년도_{\"투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "QezrUZnAn8qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 팀 데이터 크롤링_투수"
      ],
      "metadata": {
        "id": "pNYNcX0GsxZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "team_batter = 'http://www.statiz.co.kr/stat.php?opt=0&sopt=0&re=0&ys={}&ye={}&se=0&te=&tm=&ty=0&qu=auto&po=0&as=&ae=&hi=&un=&pl=&da=1&o1=WAR_ALL_ADJ&o2=TPA&de=1&lr=5&tr=&cv=&ml=1&sn=30&si=&cn='"
      ],
      "metadata": {
        "id": "YufAmOWkpCnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_batter = team_batter.format(2017,2017)\n",
        "driver.get(team_batter)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2017}년도_{\"팀타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "yA31Sq33s3iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_batter = team_batter.format(2018,2018)\n",
        "driver.get(team_batter)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2018}년도_{\"팀타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "U_h520BptVh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_batter = team_batter.format(2019,2019)\n",
        "driver.get(team_batter)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2019}년도_{\"팀타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "d3UZOxaNtasU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_batter = team_batter.format(2020,2020)\n",
        "driver.get(team_batter)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2020}년도_{\"팀타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "bLqO37wstdZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_batter = team_batter.format(2021,2021)\n",
        "driver.get(team_batter)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2021}년도_{\"팀타자\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "4jUBpqIfthJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 팀 크롤링 - 투수"
      ],
      "metadata": {
        "id": "Cu0RzC5LuCW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "team_pitcher = 'http://www.statiz.co.kr/stat.php?opt=0&sopt=0&re=1&ys={}&ye={}&se=0&te=&tm=&ty=0&qu=auto&po=0&as=&ae=&hi=&un=&pl=&da=1&o1=WAR&o2=OutCount&de=1&lr=5&tr=&cv=&ml=1&sn=30&si=&cn='"
      ],
      "metadata": {
        "id": "1Nuj1vPkuAo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_pitcher = team_pitcher.format(2017,2017)\n",
        "driver.get(team_pitcher)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2017}년도_{\"팀투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "TvDYoc5juNnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_pitcher = team_pitcher.format(2018,2018)\n",
        "driver.get(team_pitcher)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2018}년도_{\"팀투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "Rp3df5rquU-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_pitcher = team_pitcher.format(2019,2019)\n",
        "driver.get(team_pitcher)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2019}년도_{\"팀투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "bNaCdzYtuVeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_pitcher = team_pitcher.format(2020,2020)\n",
        "driver.get(team_pitcher)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2020}년도_{\"팀투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "FzX1xs9RubcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_pitcher = team_pitcher.format(2021,2021)\n",
        "driver.get(team_pitcher)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'html.parser')\n",
        "data = soup.find('table',{'id': 'mytable'})\n",
        "table = parser_functions.make2d(data)\n",
        "df = pd.DataFrame(data = table)\n",
        "df.to_csv(f'{2021}년도_{\"팀투수\"}_데이터.csv',encoding = 'CP949')"
      ],
      "metadata": {
        "id": "RvsIEvZQuezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2022 정규시즌 크롤링"
      ],
      "metadata": {
        "id": "q9UaK0pjyxNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = driver.get('https://www.koreabaseball.com/Schedule/Schedule.aspx')\n",
        "\n",
        "#정규 시즌 콤보 박스 클릭\n",
        "driver.find_element_by_xpath(\"//select[@id='ddlSeries']/option[text()='KBO 정규시즌 일정']\").click()\n",
        "\n",
        "# 전부 선택\n",
        "driver.find_element_by_xpath(\"//ul[@class='tab-schedule']/li[@class = 'all on']\").click()\n",
        "\n",
        "#5~10월 정규시즌 일정\n",
        "list_month = ['04','05','06','07','08','09']\n",
        "searchList = []\n",
        "for month in list_month:\n",
        "  #달력 선택\n",
        "  driver.find_element_by_xpath(\"//select[@id='ddlMonth']/option[text()='\"+str(month)+\"']\").click()\n",
        "  #결과\n",
        "  html = driver.page_source\n",
        "  soup = bs(html,'html.parser')\n",
        "  tblSchedule = soup.find('table',{'class':'tbl'})\n",
        "  trs = tblSchedule.find_all('tr')\n",
        "  for idx,tr in enumerate(trs):\n",
        "    if idx > 0:\n",
        "      tds = tr.find_all('td')\n",
        "      temp = [tds[0].text.strip(),tds[1].text.strip(), tds[2].text.strip(),tds[3].text.strip(),tds[7].text.strip()]\n",
        "      searchList.append(temp)\n",
        "\n",
        "#csv 만들기\n",
        "data = pd.DataFrame(searchList)\n",
        "data.columns = ['day','time','play','relay','park']\n",
        "data.head()\n",
        "data.to_csv('2022년_정규시즌_일정_0606.csv',encoding = 'CP949')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFKrGc7YywyL",
        "outputId": "094fef52-6486-4a02-8bb7-a7ff157946dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x-gjZGzpWX-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}